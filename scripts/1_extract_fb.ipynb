{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "806e32d6",
   "metadata": {},
   "source": [
    "# 1. Script for extracting FB data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3ed6c",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facebook_scraper import get_posts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884612bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b7198b",
   "metadata": {},
   "source": [
    "## Extracting comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9f1e0",
   "metadata": {},
   "source": [
    "### Import selected FB posts data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b1328",
   "metadata": {},
   "source": [
    "As mentioned in the script to extract Reddit data, I was only interested in extracting posts from Facebook and Reddit that contained the same article from Straits Times/Channel News Asia reporting on Ministry of Health (MOH) announcements about imposition/tightening of restrictions. The links to Facebook posts with matching Reddit posts were manually input into `posts_reddit_fb_selected.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00998a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_selected = pd.read_csv('../data/posts_reddit_fb_selected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563326f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fb_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c90f8",
   "metadata": {},
   "source": [
    "Since there are repeated links in the `full_link_fb` column, I will get the set of unique links and output it as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_links = list(fb_selected['full_link_fb'].unique())\n",
    "print(len(fb_links))\n",
    "print(fb_links[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1b10a",
   "metadata": {},
   "source": [
    "### Get comments for each post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57609de4",
   "metadata": {},
   "source": [
    "I used the [facebook-scraper Python package](https://pypi.org/project/facebook-scraper/) to extract all the comments from each post using the URL for the post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ccd3b",
   "metadata": {},
   "source": [
    "To better understand how the results were structured, I extracted comments from the first post in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73242f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import logging\n",
    "\n",
    "# from facebook_scraper import get_posts, enable_logging\n",
    "\n",
    "# enable_logging(logging.DEBUG)\n",
    "# logging.basicConfig(filename=\"logs.txt\", filemode='w', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affed940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posts = get_posts(post_urls=[fb_links[0]], \n",
    "                  cookies='from_browser', \n",
    "                  options={'comments': True, 'allow_extra_requests': False, 'posts_per_page': 200})\n",
    "\n",
    "for p in posts:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffad5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posts = get_posts(post_urls=[fb_links[0]], \n",
    "                  cookies='from_browser', \n",
    "                  options={'comments': True, 'allow_extra_requests': False, 'posts_per_page': 200})\n",
    "\n",
    "for p in posts:\n",
    "    print(p['comments'])\n",
    "    print(len(p['comments_full']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ee16e",
   "metadata": {},
   "source": [
    "From the above we can see that:\n",
    "- According to the `comments` parameter, there are 502 comments for the first post.\n",
    "- Data on each comment is a nested json under the `comments_full` parameter. There are only 252 comments in `comments_full`, so these are top-level comments and the rest of the comments are comment replies nested in the `replies` parameter under each comment in `comments_full`.\n",
    "- Comment replies have to be extracted by iterating through each comment in `comments_full`, but this will be extremely time consuming and tricky as Facebook has tight restrictions on scraping behaviour that makes it necessary to introduce long sleep times between each comment/reply extraction to prevent account banning. As such, I will **not** extract comment replies and only extract top level comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a990b4d",
   "metadata": {},
   "source": [
    "The following code extracts comments from the 29 Facebook posts, 5 posts at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4e8cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments = {'comment_id': [], 'text': []}\n",
    "\n",
    "idx = 0\n",
    "while (idx < 5):\n",
    "    sample = [fb_links[idx]]\n",
    "    idx = idx+1\n",
    "    for post in get_posts(post_urls=sample,\n",
    "                          cookies='from_browser',\n",
    "                          timeout=180,\n",
    "                          options={'comments': 'generator', 'progress': True, 'allow_extra_requests': False, 'posts_per_page': 200}):\n",
    "        \n",
    "        comments_full = post['comments_full']\n",
    "        \n",
    "        for comment in comments_full:\n",
    "            comments['comment_id'].append(str(comment['comment_id']))\n",
    "            comments['text'].append(str(comment['comment_text']))\n",
    "            sleep(3)\n",
    "    \n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f97d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df_1 = pd.DataFrame(comments)\n",
    "comments_df_1.to_csv('../data/fb_comments_1.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2041ffa",
   "metadata": {},
   "source": [
    "### Concatenating all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a97209",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df_1 = pd.read_csv('../data/comments_fb_1.csv', index_col=0)\n",
    "comments_df_2 = pd.read_csv('../data/comments_fb_2.csv', index_col=0)\n",
    "comments_df_3 = pd.read_csv('../data/comments_fb_3.csv', index_col=0)\n",
    "comments_df_4 = pd.read_csv('../data/comments_fb_4.csv', index_col=0)\n",
    "comments_df_5 = pd.read_csv('../data/comments_fb_5.csv', index_col=0)\n",
    "comments_df_6 = pd.read_csv('../data/comments_fb_6.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad081c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df_list = [comments_df_1, comments_df_2, comments_df_3, comments_df_4, comments_df_5, comments_df_6, comments_df_7,\n",
    "                   comments_df_8, comments_df_9]\n",
    "comments_dfs = pd.concat(comments_df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f382f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dfs.to_csv('../data/comments_fb_all.csv', encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
